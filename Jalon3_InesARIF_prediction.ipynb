{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhFvNYIPkqPt",
        "outputId": "b2fb4826-3f3c-4c8c-a7de-fecace215e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (7.9.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->import-ipynb) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.11.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.16.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.13.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import-ipynb) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import-ipynb) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (22.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "import pickle\n",
        "import import_ipynb\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import unidecode\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import num2words\n",
        "\n",
        "from autocorrect import Speller\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import contractions\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4XxpfdnlJiW",
        "outputId": "79f31135-307c-449d-ce32-5a4dc434bfca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVRbBRFwnNyx",
        "outputId": "2aa95efa-a56e-4ef7-c496-7511bd30ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Texte en minuscule\n",
        "def lower_case_convertion(text):\n",
        "\tlower_text = text.lower()\n",
        "\treturn lower_text\n",
        "\n",
        "### Suppression des tags HTML\n",
        "def remove_html_tags(text):\n",
        "\thtml_pattern = r'<.*?>'\n",
        "\twithout_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
        "\treturn without_html\n",
        "\n",
        "# Suppression des URLS\n",
        "def remove_urls(text):\n",
        "\turl_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "\twithout_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
        "\treturn without_urls\n",
        "\n",
        "#ASCII\n",
        "def accented_to_ascii(text):\n",
        "\n",
        "\t# apply unidecode function on text to convert\n",
        "\t# accented characters to ASCII values\n",
        "\ttext = unidecode.unidecode(text)\n",
        "\treturn text\n",
        "\n",
        "### Contraction\n",
        "def expand_contractions(text):\n",
        "  ### We can use the dict : https://stackoverflow.com/questions/60901735/importerror-cannot-import-name-contraction-map-from-contractions\n",
        "\treturn contractions.fix(text)\n",
        "\n",
        "\n",
        "#######################################################################################################################################\n",
        "\n",
        "### Suppresion de la ponctuation \n",
        "import string\n",
        "\n",
        "def remove_punctuation(text) :\n",
        "  return text.translate(str.maketrans('', '', string.punctuation))\n"
      ],
      "metadata": {
        "id": "PzaE6QF70oW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text) :\n",
        "  text = lower_case_convertion(text)\n",
        "  text = remove_html_tags(text)\n",
        "  text = remove_urls(text)\n",
        "  text = accented_to_ascii(text)\n",
        "  text =  expand_contractions(text)\n",
        "  text = remove_punctuation(text)\n",
        "  return text "
      ],
      "metadata": {
        "id": "9O_0tP-B0p3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_words(sentence_tokenized):\n",
        "\n",
        "    keywordSet = {\"never\", \"nothing\", \"nowhere\", \"none\", \"not\"}\n",
        "\n",
        "    for word in sentence_tokenized:\n",
        "      if (word in keywordSet) and (sentence_tokenized.index(word) < len(sentence_tokenized)-1):\n",
        "        sentence_tokenized[sentence_tokenized.index(word)+1]=sentence_tokenized[sentence_tokenized.index(word)+1] + '_NEG'\n",
        "        sentence_tokenized.pop(sentence_tokenized.index(word))\n",
        "    return sentence_tokenized\n",
        "\n"
      ],
      "metadata": {
        "id": "e9zklexk0v11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "def remove_stop_word(text) :\n",
        "  stop_words = stopwords.words('english')\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens_without_sw = [word for word in tokens if not word in stop_words]\n",
        "  text = ' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mfFF7xX1Mgg",
        "outputId": "90492a2c-d699-40cc-8d91-1110ba9d2b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def merge(dict1, dict2):\n",
        "    for i in dict2.keys():\n",
        "        dict1[i]=dict2[i]\n",
        "    return dict1\n",
        "    \n",
        "def pos_tagging(text):\n",
        "  ### POS_TAGGING\n",
        "  lmtzr = WordNetLemmatizer()\n",
        "  all_tag_words = {}\n",
        "  tokens = word_tokenize(text)\n",
        "  tagged = dict(nltk.pos_tag(tokens))\n",
        "  all_tag_words = merge(tagged, all_tag_words)\n",
        "  return all_tag_words\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# initialize lemmatizer object\n",
        "lemma = WordNetLemmatizer()  \n",
        "### Lemmatize with POS tagging \n",
        "def lemmatize(text, all_tag_words):\n",
        "  tokens = ''.join(text).split()\n",
        "  for i,token in enumerate(tokens) :\n",
        "    try :\n",
        "      tokens[i] = lemma.lemmatize(token,get_wordnet_pos(all_tag_words[token]))\n",
        "    except : \n",
        "      pass\n",
        "  text = ' '.join(tokens)\n",
        "  return text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx9dsq-M1hyy",
        "outputId": "c03aa363-f63e-4799-9ff5-d2b011ec9de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {\n",
        "    0 : \"bad customer service at phone\",\n",
        "    1 : \"disappointed taste\",\n",
        "    2 : \"cold pizza\",\n",
        "    3 : \"not enough chicken\",\n",
        "    4 : \"bad food quality\",\n",
        "    5 : \"bad service\",\n",
        "    6 : \"bad burgers\",\n",
        "    7 : \"long wait\",\n",
        "    8 : \"bad experience on all levels\",\n",
        "    9 : \"bad experience at the bar\",\n",
        "    10 : \"problem with the delivery or the online order\",\n",
        "    11 : \"bad experience several times\",\n",
        "    12 : \"unorganized staff\",\n",
        "    13 : \"poor quality sushis\",\n",
        "    14 : \"dangerous place\"\n",
        "}"
      ],
      "metadata": {
        "id": "4oktXb9H6iR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Sitou/model_entraîné_InesArif.pickle', 'rb') as f:\n",
        "    model = pickle.load(f) \n",
        "\n",
        "with open('/content/drive/MyDrive/Sitou/vectorizer_InesArif.pickle', 'rb') as f:\n",
        "    vectorizer = pickle.load(f) \n"
      ],
      "metadata": {
        "id": "6ybryCKbp62k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text : str,nb_features:int,labels_dict=labels_dict, blob=True) :\n",
        "\n",
        "  if blob == True :\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    print(polarity)\n",
        "    if polarity > 0 :\n",
        "      return polarity, None\n",
        "\n",
        "    if polarity<0 :\n",
        "      text = preprocessing(text)\n",
        "      dict_pos = pos_tagging(text)\n",
        "      text = lemmatize(text, dict_pos)\n",
        "      text = [text]\n",
        "      vect = vectorizer.transform(text)      \n",
        "      nmf_features = list(list(model.transform(vect))[0])\n",
        "      nmf_features_copy = nmf_features\n",
        "      indexes = []\n",
        "      indexes.append(nmf_features.index(max(nmf_features_copy)))\n",
        "      nmf_features_copy.pop(indexes[0])\n",
        "      \n",
        "      for i in range(nb_features-1):\n",
        "        \n",
        "        index = nmf_features.index(max(nmf_features_copy))\n",
        "        indexes.append(index)\n",
        "        nmf_features_copy.pop(index)\n",
        "\n",
        "      for i in indexes :\n",
        "        print(labels_dict[i])\n",
        "      \n",
        "      topics = []\n",
        "      for i in indexes:\n",
        "        topics.append(labels_dict[i])\n",
        "      return polarity, topics"
      ],
      "metadata": {
        "id": "gsD50ErNlT8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}